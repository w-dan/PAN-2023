{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch transformers numpy pandas sentence-transformers -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bl1tty\\Documents\\Uni\\PAN-2023\\PAN-NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_FOLDER = \"PAN2020-authorship-verification\"\n",
    "DATASET1_TRAIN = \"pan20-authorship-verification-training-small/pan20-authorship-verification-training-small/pan20-authorship-verification-training-small-truth.jsonl\"\n",
    "DATASET2_TRAIN = \"pan20-authorship-verification-training-small/pan20-authorship-verification-training-small/pan20-authorship-verification-training-small.jsonl\"\n",
    "# DATASET1_TRAIN = \"pan20-authorship-verification-training-small-truth.jsonl\"\n",
    "# DATASET2_TRAIN = \"pan20-authorship-verification-training-small.jsonl\"\n",
    "FILE_PATH_1 = f'./{PARENT_FOLDER}/{DATASET1_TRAIN}'\n",
    "FILE_PATH_2 = f'./{PARENT_FOLDER}/{DATASET2_TRAIN}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_from_file (file_path : str) -> List:\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                parsed_data = json.loads(line)\n",
    "                data.append(parsed_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON: {e}\")\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "df_ground_truth = get_dataframe_from_file(FILE_PATH_1)\n",
    "df_inputs = get_dataframe_from_file(FILE_PATH_2)\n",
    "\n",
    "df_combined = pd.merge(df_ground_truth, df_inputs, on='id')\n",
    "\n",
    "######################\n",
    "#     CUIDADO!!!!!   #\n",
    "######################\n",
    "df_combined = df_combined.head(32)\n",
    "print(len(df_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>same</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6cced668-6e51-5212-873c-717f2bc91ce6</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c6c188a-db28-59aa-8c09-3d0f799ff579</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0cfa94f-c9ec-5aa5-8331-a5a249b664cf</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e6e86e73-9a7b-58f2-a652-a17b4a1bcabf</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fe541af-912e-5a86-81a5-94c6d3891509</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  same             authors\n",
       "0  6cced668-6e51-5212-873c-717f2bc91ce6  True  [1446633, 1446633]\n",
       "1  3c6c188a-db28-59aa-8c09-3d0f799ff579  True  [1446633, 1446633]\n",
       "2  b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  True  [1446633, 1446633]\n",
       "3  e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  True  [1446633, 1446633]\n",
       "4  4fe541af-912e-5a86-81a5-94c6d3891509  True  [1446633, 1446633]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52601"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_not_nulls(df: pd.DataFrame) -> None:\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicate_ids(df: pd.DataFrame) -> pd.Series:\n",
    "    # Find duplicate IDs\n",
    "    duplicate_ids = df[df.duplicated(subset=['id'], keep=False)]\n",
    "\n",
    "    # Calculate the sum of repetitions\n",
    "    sum_repetitions = len(duplicate_ids)\n",
    "\n",
    "    return sum_repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         0\n",
      "same       0\n",
      "authors    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_not_nulls(df_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         0\n",
      "fandoms    0\n",
      "pair       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_not_nulls(df_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "- Robust dataset: Separate pairs and with its fandoms. Use fandoms to generate new dataset of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert count_duplicate_ids(df_ground_truth) == count_duplicate_ids(df_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(df_combined) - len(df_inputs) == 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina la columna \"same\" ya que no da información relevante para el entrenamiento del modelo. Debido a que es una comparación entre dos ids que son las salidas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.drop(\"authors\", axis=1).drop(\"fandoms\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename \"authors\" to \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.rename(columns={'same': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "      <th>pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6cced668-6e51-5212-873c-717f2bc91ce6</td>\n",
       "      <td>True</td>\n",
       "      <td>[I shift a bit, warily letting my eyes dart fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c6c188a-db28-59aa-8c09-3d0f799ff579</td>\n",
       "      <td>True</td>\n",
       "      <td>[I shift a bit, warily letting my eyes dart fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0cfa94f-c9ec-5aa5-8331-a5a249b664cf</td>\n",
       "      <td>True</td>\n",
       "      <td>[A single tear escaped me as I left. I did hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e6e86e73-9a7b-58f2-a652-a17b4a1bcabf</td>\n",
       "      <td>True</td>\n",
       "      <td>[\"Ja.\" Ludwig kept his gaze upon her, solidly....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fe541af-912e-5a86-81a5-94c6d3891509</td>\n",
       "      <td>True</td>\n",
       "      <td>[And he did. Slowly, hesitantly...but coming f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     y  \\\n",
       "0  6cced668-6e51-5212-873c-717f2bc91ce6  True   \n",
       "1  3c6c188a-db28-59aa-8c09-3d0f799ff579  True   \n",
       "2  b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  True   \n",
       "3  e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  True   \n",
       "4  4fe541af-912e-5a86-81a5-94c6d3891509  True   \n",
       "\n",
       "                                                pair  \n",
       "0  [I shift a bit, warily letting my eyes dart fr...  \n",
       "1  [I shift a bit, warily letting my eyes dart fr...  \n",
       "2  [A single tear escaped me as I left. I did hav...  \n",
       "3  [\"Ja.\" Ludwig kept his gaze upon her, solidly....  \n",
       "4  [And he did. Slowly, hesitantly...but coming f...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   6cced668-6e51-5212-873c-717f2bc91ce6\n",
       "y                                                    True\n",
       "pair    [I shift a bit, warily letting my eyes dart fr...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[['text1', 'text2']] = df_combined['pair'].apply(pd.Series)\n",
    "df_combined = df_combined.drop(\"pair\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6cced668-6e51-5212-873c-717f2bc91ce6</td>\n",
       "      <td>True</td>\n",
       "      <td>I shift a bit, warily letting my eyes dart fro...</td>\n",
       "      <td>\"All will become one with Russia,\" he said, al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c6c188a-db28-59aa-8c09-3d0f799ff579</td>\n",
       "      <td>True</td>\n",
       "      <td>I shift a bit, warily letting my eyes dart fro...</td>\n",
       "      <td>Suddenly, a piece of ice falls into the pit of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0cfa94f-c9ec-5aa5-8331-a5a249b664cf</td>\n",
       "      <td>True</td>\n",
       "      <td>A single tear escaped me as I left. I did have...</td>\n",
       "      <td>got the Yang yoyo.\" Kimiko pulled the other ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e6e86e73-9a7b-58f2-a652-a17b4a1bcabf</td>\n",
       "      <td>True</td>\n",
       "      <td>\"Ja.\" Ludwig kept his gaze upon her, solidly. ...</td>\n",
       "      <td>SilverGray lll...YellowRagged llll...GrayMilli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fe541af-912e-5a86-81a5-94c6d3891509</td>\n",
       "      <td>True</td>\n",
       "      <td>And he did. Slowly, hesitantly...but coming fr...</td>\n",
       "      <td>\"Let\"s go,\" Raimondo said and then started in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     y  \\\n",
       "0  6cced668-6e51-5212-873c-717f2bc91ce6  True   \n",
       "1  3c6c188a-db28-59aa-8c09-3d0f799ff579  True   \n",
       "2  b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  True   \n",
       "3  e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  True   \n",
       "4  4fe541af-912e-5a86-81a5-94c6d3891509  True   \n",
       "\n",
       "                                               text1  \\\n",
       "0  I shift a bit, warily letting my eyes dart fro...   \n",
       "1  I shift a bit, warily letting my eyes dart fro...   \n",
       "2  A single tear escaped me as I left. I did have...   \n",
       "3  \"Ja.\" Ludwig kept his gaze upon her, solidly. ...   \n",
       "4  And he did. Slowly, hesitantly...but coming fr...   \n",
       "\n",
       "                                               text2  \n",
       "0  \"All will become one with Russia,\" he said, al...  \n",
       "1  Suddenly, a piece of ice falls into the pit of...  \n",
       "2  got the Yang yoyo.\" Kimiko pulled the other ha...  \n",
       "3  SilverGray lll...YellowRagged llll...GrayMilli...  \n",
       "4  \"Let\"s go,\" Raimondo said and then started in ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.iloc[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21336"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_length = 0\n",
    "for i in range(len(df_combined)):\n",
    "    mean_length += len(df_combined.iloc[i, 2]) + len(df_combined.iloc[i, 3])\n",
    "\n",
    "mean_length /= len(df_combined) * 2\n",
    "mean_length = int(mean_length)\n",
    "mean_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, model_name, max_len=512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.data = df\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded_input_text1 = self.tokenizer(self.data.iloc[index, 2], max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoded_input_text2 = self.tokenizer(self.data.iloc[index, 3], max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        print(encoded_input_text1['input_ids'].shape)\n",
    "\n",
    "        return {\n",
    "            \"encoded_input_text1\": encoded_input_text1,\n",
    "            \"encoded_input_text2\": encoded_input_text2,\n",
    "            \"targets\": torch.tensor(int(self.data.iloc[index, 1]), dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer without woth pairs\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, model_name, freeze_transformer):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        if freeze_transformer:\n",
    "            for param in self.transformer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.dense1 = nn.Linear(768, 512)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.cosine = nn.CosineSimilarity(dim=1)\n",
    "        self.dense = nn.Linear(1, 1)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, encoded_input_text1, encoded_input_text2):\n",
    "        print(f\"input_ids shape : {encoded_input_text1['input_ids'].shape}\")\n",
    "\n",
    "        model_output_text1 = self.transformer(\n",
    "            input_ids=encoded_input_text1['input_ids'][:, 0, :],\n",
    "            attention_mask=encoded_input_text1['attention_mask'][:, 0, :],\n",
    "        ).last_hidden_state[:, 0]\n",
    "        model_output_text2 = self.transformer(\n",
    "            input_ids=encoded_input_text2['input_ids'][:, 0, :],\n",
    "            attention_mask=encoded_input_text2['attention_mask'][:, 0, :],\n",
    "        ).last_hidden_state[:, 0]\n",
    "\n",
    "        x_a, x_b = self.dense1(model_output_text1), self.dense1(model_output_text2)\n",
    "        x_a, x_b = self.gelu(self.dropout(x_a)), self.gelu(self.dropout(x_b))\n",
    "        sem_sim = self.cosine(x_a, x_b)\n",
    "        print(sem_sim)\n",
    "\n",
    "        return self.sigmoid(sem_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mio para comprobar que funciona y corre el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AnnaWegmann/Style-Embedding' # 'bert-base-uncased'  # Choose the appropriate pretrained model #'AnnaWegmann/Style-Embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_combined, test_size=0.2, random_state=42)\n",
    "train_dataset = CustomDataset(train_df, model_name, max_len=mean_length)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([25, 12,  0,  4, 16,  5, 13, 11, 23,  1,  2, 26,  3, 21, 27, 22, 18, 31,\n",
      "       20,  7, 10, 14, 28, 19,  6],\n",
      "      dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small test to see that everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "input_ids shape : torch.Size([1, 1, 512])\n",
      "tensor([0.4809], grad_fn=<SumBackward1>)\n",
      "tensor([0.6180], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# anna weinman style embeddings - hard negative mininng\n",
    "model = TransformerModel(model_name=model_name, freeze_transformer=True)\n",
    "model.train() # tell model we are going to train -> https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "\n",
    "for batch in train_data_loader:\n",
    "    x = model.forward(batch[\"encoded_input_text1\"], batch[\"encoded_input_text2\"])\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model\n",
    "\n",
    "See diapos a partir de la 152 y usar anotación de la diapos (ejemplo: bs_sl -> Batch size - Sequence Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "input_ids shape : torch.Size([16, 1, 512])\n",
      "tensor([0.4793, 0.4091, 0.4078, 0.5032, 0.2519, 0.5472, 0.3541, 0.4513, 0.4668,\n",
      "        0.4088, 0.4124, 0.4748, 0.3089, 0.5494, 0.4951, 0.3427],\n",
      "       grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "input_ids shape : torch.Size([9, 1, 512])\n",
      "tensor([0.4436, 0.5873, 0.6255, 0.5425, 0.5589, 0.5806, 0.5474, 0.5325, 0.4845],\n",
      "       grad_fn=<SumBackward1>)\n",
      "Model weights saved to model_epoch_1.pt\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "input_ids shape : torch.Size([7, 1, 512])\n",
      "tensor([0.9249, 0.7616, 0.9149, 0.9699, 0.8712, 0.9841, 0.9753])\n",
      "predictions (real): tensor([0.7160, 0.6817, 0.7140, 0.7251, 0.7050, 0.7279, 0.7262])\n",
      "predictions: tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "ground_truth: tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch [1/10], Validation Loss: 0.33751001954078674, Accuracy: 1.0\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "input_ids shape : torch.Size([16, 1, 512])\n",
      "tensor([0.5967, 0.6255, 0.3978, 0.3347, 0.4306, 0.4664, 0.4103, 0.3985, 0.4980,\n",
      "        0.4088, 0.6673, 0.6462, 0.5978, 0.5440, 0.4273, 0.5995],\n",
      "       grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "input_ids shape : torch.Size([9, 1, 512])\n",
      "tensor([0.5255, 0.3124, 0.5933, 0.5104, 0.3605, 0.6529, 0.5527, 0.5750, 0.5334],\n",
      "       grad_fn=<SumBackward1>)\n",
      "Model weights saved to model_epoch_2.pt\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "input_ids shape : torch.Size([7, 1, 512])\n",
      "tensor([0.8573, 0.9248, 0.9916, 0.9846, 0.9525, 0.9883, 0.9523])\n",
      "predictions (real): tensor([0.7021, 0.7160, 0.7294, 0.7280, 0.7216, 0.7287, 0.7216])\n",
      "predictions: tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "ground_truth: tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch [2/10], Validation Loss: 0.3270938992500305, Accuracy: 1.0\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "input_ids shape : torch.Size([16, 1, 512])\n",
      "tensor([0.5921, 0.5301, 0.5237, 0.5320, 0.5139, 0.6930, 0.6721, 0.6112, 0.6818,\n",
      "        0.6478, 0.3944, 0.5860, 0.5621, 0.5423, 0.5989, 0.5882],\n",
      "       grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "input_ids shape : torch.Size([9, 1, 512])\n",
      "tensor([0.5669, 0.5315, 0.4826, 0.7012, 0.5687, 0.5653, 0.6529, 0.5187, 0.6365],\n",
      "       grad_fn=<SumBackward1>)\n",
      "Model weights saved to model_epoch_3.pt\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "input_ids shape : torch.Size([7, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(model_name=model_name, freeze_transformer=True)\n",
    "\n",
    "# Define your loss function (customize based on your task)\n",
    "criterion = nn.BCELoss()  # Example: Binary Cross Entropy\n",
    "\n",
    "# Define optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "train_df, val_df = train_test_split(df_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            encoded_input_text1 = batch['encoded_input_text1']\n",
    "            encoded_input_text2 = batch['encoded_input_text2']\n",
    "            targets = batch['targets']\n",
    "\n",
    "            y_pred = model.forward(encoded_input_text1, encoded_input_text2)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = (y_pred > 0.5).float()  # Assuming a binary classification task\n",
    "            correct_predictions += (predictions == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "\n",
    "    print(f\"predictions (real): {y_pred}\")\n",
    "    print(f\"predictions: {predictions}\")\n",
    "    print(f\"ground_truth: {targets}\")\n",
    "\n",
    "    return average_loss, accuracy\n",
    "\n",
    "\n",
    "def training_step(encoded_input_text1, encoded_input_text2, targets, model, optimizer, criterion):\n",
    "    # !!!! necessary to set the model to training mode before\n",
    "    \n",
    "    # forward pass\n",
    "    y_pred = model.forward(encoded_input_text1, encoded_input_text2)\n",
    "\n",
    "    loss = criterion(y_pred, targets)\n",
    "    \n",
    "    # baccpropagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = CustomDataset(train_df, model_name)\n",
    "validate_dataset = CustomDataset(val_df, model_name)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(train_data_loader):\n",
    "        input_text1 = batch['encoded_input_text1']\n",
    "        input_text2 = batch['encoded_input_text2']\n",
    "        targets = batch['targets']\n",
    "\n",
    "        # print(f\"On loop {input_text1.shape}\")\n",
    "        loss = training_step(input_text1, input_text2, targets, model, optimizer, criterion)\n",
    "        running_loss += loss\n",
    "\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                  f\"Step [{i + 1}/{len(train_data_loader)}], \"\n",
    "                  f\"Loss: {running_loss / 100}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Save the model weights after each epoch\n",
    "    checkpoint_path = f\"model_epoch_{epoch + 1}.pt\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Model weights saved to {checkpoint_path}\")\n",
    "\n",
    "    # Evaluate the model on the validation set after each epoch\n",
    "    val_loss, val_accuracy = evaluate(model, val_data_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss}, Accuracy: {val_accuracy}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUEDA:\n",
    "\n",
    "1. El validate-test serían pasar en un bucle el forward del otro zip y capturar resultados para conseguir las métricas\n",
    "2. Fine tunning (mínimo)\n",
    "3. Escribir cosas\n",
    "\n",
    "PD: Quitar el print de los shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = f\"{PARENT_FOLDER}/pan20-authorship-verification-test/pan20-authorship-verification-test\"\n",
    "VALUES_FILE = \"pan20-authorship-verification-test.jsonl\"\n",
    "GROUND_TRUTH = \"pan20-authorship-verification-test-truth.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = get_dataframe_from_file(f\"{FOLDER}/{GROUND_TRUTH}\")\n",
    "df_inputs = get_dataframe_from_file(f\"{FOLDER}/{VALUES_FILE}\")\n",
    "\n",
    "df_combined_val = pd.merge(df_ground_truth, df_inputs, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_val = df_combined.drop(\"authors\", axis=1).drop(\"fandoms\", axis=1)\n",
    "df_combined_val = df_combined_val.rename(columns={'same': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_val.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_val[['text1', 'text2']] = df_combined_val['pair'].apply(pd.Series)\n",
    "df_combined_val = df_combined_val.drop(\"pair\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model_epoch_10.pt\"))\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_dataset = CustomDataset(train_df, model_name)\n",
    "validate_dataset = CustomDataset(val_df, model_name)\n",
    "\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_data_loader):\n",
    "        input_text1 = batch['encoded_input_text1']\n",
    "        input_text2 = batch['encoded_input_text2']\n",
    "        targets = batch['targets']\n",
    "\n",
    "        outputs = model.forward(input_text1, input_text2)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {100 * accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
