{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install -q torch transformers numpy pandas sentence-transformers -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/jupyter-nbextension\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('notebook==6.4.8', 'console_scripts', 'jupyter-nbextension')())\n",
      "  File \"/usr/lib/python3/dist-packages/jupyter_core/application.py\", line 264, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/lib/python3/dist-packages/notebook/nbextensions.py\", line 980, in start\n",
      "    super().start()\n",
      "  File \"/usr/lib/python3/dist-packages/jupyter_core/application.py\", line 253, in start\n",
      "    self.subapp.start()\n",
      "  File \"/usr/lib/python3/dist-packages/notebook/nbextensions.py\", line 888, in start\n",
      "    self.toggle_nbextension_python(self.extra_args[0])\n",
      "  File \"/usr/lib/python3/dist-packages/notebook/nbextensions.py\", line 861, in toggle_nbextension_python\n",
      "    return toggle(module,\n",
      "  File \"/usr/lib/python3/dist-packages/notebook/nbextensions.py\", line 474, in enable_nbextension_python\n",
      "    return _set_nbextension_state_python(True, module, user, sys_prefix,\n",
      "  File \"/usr/lib/python3/dist-packages/notebook/nbextensions.py\", line 373, in _set_nbextension_state_python\n",
      "    return [_set_nbextension_state(section=nbext[\"section\"],\n",
      "  File \"/usr/lib/python3/dist-packages/notebook/nbextensions.py\", line 373, in <listcomp>\n",
      "    return [_set_nbextension_state(section=nbext[\"section\"],\n",
      "  File \"/usr/lib/python3/dist-packages/notebook/nbextensions.py\", line 343, in _set_nbextension_state\n",
      "    cm.update(section, {\"load_extensions\": {require: state}})\n",
      "  File \"/usr/lib/python3/dist-packages/notebook/config_manager.py\", line 131, in update\n",
      "    self.set(section_name, data)\n",
      "  File \"/usr/lib/python3/dist-packages/notebook/config_manager.py\", line 108, in set\n",
      "    self.ensure_config_dir_exists()\n",
      "  File \"/usr/lib/python3/dist-packages/notebook/config_manager.py\", line 65, in ensure_config_dir_exists\n",
      "    os.makedirs(self.config_dir, 0o755)\n",
      "  File \"/usr/lib/python3.10/os.py\", line 215, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.10/os.py\", line 215, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.10/os.py\", line 225, in makedirs\n",
      "    mkdir(name, mode)\n",
      "PermissionError: [Errno 13] Permission denied: '/usr/etc'\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_FOLDER = \"PAN2020-authorship-verification\"\n",
    "DATASET1_TRAIN = \"pan20-authorship-verification-training-small/pan20-authorship-verification-training-small-truth.jsonl\"\n",
    "DATASET2_TRAIN = \"pan20-authorship-verification-training-small/pan20-authorship-verification-training-small.jsonl\"\n",
    "FILE_PATH_1 = f'{PARENT_FOLDER}/{DATASET1_TRAIN}'\n",
    "FILE_PATH_2 = f'{PARENT_FOLDER}/{DATASET2_TRAIN}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_from_file (file_path : str) -> List:\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                parsed_data = json.loads(line)\n",
    "                data.append(parsed_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON: {e}\")\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = get_dataframe_from_file(FILE_PATH_1)\n",
    "df_inputs = get_dataframe_from_file(FILE_PATH_2)\n",
    "\n",
    "df_combined = pd.merge(df_ground_truth, df_inputs, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>same</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6cced668-6e51-5212-873c-717f2bc91ce6</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c6c188a-db28-59aa-8c09-3d0f799ff579</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0cfa94f-c9ec-5aa5-8331-a5a249b664cf</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e6e86e73-9a7b-58f2-a652-a17b4a1bcabf</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fe541af-912e-5a86-81a5-94c6d3891509</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  same             authors\n",
       "0  6cced668-6e51-5212-873c-717f2bc91ce6  True  [1446633, 1446633]\n",
       "1  3c6c188a-db28-59aa-8c09-3d0f799ff579  True  [1446633, 1446633]\n",
       "2  b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  True  [1446633, 1446633]\n",
       "3  e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  True  [1446633, 1446633]\n",
       "4  4fe541af-912e-5a86-81a5-94c6d3891509  True  [1446633, 1446633]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52601"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_not_nulls(df: pd.DataFrame) -> None:\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicate_ids(df: pd.DataFrame) -> pd.Series:\n",
    "    # Find duplicate IDs\n",
    "    duplicate_ids = df[df.duplicated(subset=['id'], keep=False)]\n",
    "\n",
    "    # Calculate the sum of repetitions\n",
    "    sum_repetitions = len(duplicate_ids)\n",
    "\n",
    "    return sum_repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         0\n",
      "same       0\n",
      "authors    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_not_nulls(df_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         0\n",
      "fandoms    0\n",
      "pair       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_not_nulls(df_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "- Robust dataset: Separate pairs and with its fandoms. Use fandoms to generate new dataset of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert count_duplicate_ids(df_ground_truth) == count_duplicate_ids(df_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_combined) - len(df_inputs) == 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina la columna \"same\" ya que no da información relevante para el entrenamiento del modelo. Debido a que es una comparación entre dos ids que son las salidas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.drop(\"authors\", axis=1).drop(\"fandoms\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename \"authors\" to \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.rename(columns={'same': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "      <th>pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6cced668-6e51-5212-873c-717f2bc91ce6</td>\n",
       "      <td>True</td>\n",
       "      <td>[I shift a bit, warily letting my eyes dart fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c6c188a-db28-59aa-8c09-3d0f799ff579</td>\n",
       "      <td>True</td>\n",
       "      <td>[I shift a bit, warily letting my eyes dart fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0cfa94f-c9ec-5aa5-8331-a5a249b664cf</td>\n",
       "      <td>True</td>\n",
       "      <td>[A single tear escaped me as I left. I did hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e6e86e73-9a7b-58f2-a652-a17b4a1bcabf</td>\n",
       "      <td>True</td>\n",
       "      <td>[\"Ja.\" Ludwig kept his gaze upon her, solidly....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fe541af-912e-5a86-81a5-94c6d3891509</td>\n",
       "      <td>True</td>\n",
       "      <td>[And he did. Slowly, hesitantly...but coming f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     y  \\\n",
       "0  6cced668-6e51-5212-873c-717f2bc91ce6  True   \n",
       "1  3c6c188a-db28-59aa-8c09-3d0f799ff579  True   \n",
       "2  b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  True   \n",
       "3  e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  True   \n",
       "4  4fe541af-912e-5a86-81a5-94c6d3891509  True   \n",
       "\n",
       "                                                pair  \n",
       "0  [I shift a bit, warily letting my eyes dart fr...  \n",
       "1  [I shift a bit, warily letting my eyes dart fr...  \n",
       "2  [A single tear escaped me as I left. I did hav...  \n",
       "3  [\"Ja.\" Ludwig kept his gaze upon her, solidly....  \n",
       "4  [And he did. Slowly, hesitantly...but coming f...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   6cced668-6e51-5212-873c-717f2bc91ce6\n",
       "y                                                    True\n",
       "pair    [I shift a bit, warily letting my eyes dart fr...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[['text1', 'text2']] = df_combined['pair'].apply(pd.Series)\n",
    "df_combined = df_combined.drop(\"pair\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6cced668-6e51-5212-873c-717f2bc91ce6</td>\n",
       "      <td>True</td>\n",
       "      <td>I shift a bit, warily letting my eyes dart fro...</td>\n",
       "      <td>\"All will become one with Russia,\" he said, al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c6c188a-db28-59aa-8c09-3d0f799ff579</td>\n",
       "      <td>True</td>\n",
       "      <td>I shift a bit, warily letting my eyes dart fro...</td>\n",
       "      <td>Suddenly, a piece of ice falls into the pit of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0cfa94f-c9ec-5aa5-8331-a5a249b664cf</td>\n",
       "      <td>True</td>\n",
       "      <td>A single tear escaped me as I left. I did have...</td>\n",
       "      <td>got the Yang yoyo.\" Kimiko pulled the other ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e6e86e73-9a7b-58f2-a652-a17b4a1bcabf</td>\n",
       "      <td>True</td>\n",
       "      <td>\"Ja.\" Ludwig kept his gaze upon her, solidly. ...</td>\n",
       "      <td>SilverGray lll...YellowRagged llll...GrayMilli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fe541af-912e-5a86-81a5-94c6d3891509</td>\n",
       "      <td>True</td>\n",
       "      <td>And he did. Slowly, hesitantly...but coming fr...</td>\n",
       "      <td>\"Let\"s go,\" Raimondo said and then started in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     y  \\\n",
       "0  6cced668-6e51-5212-873c-717f2bc91ce6  True   \n",
       "1  3c6c188a-db28-59aa-8c09-3d0f799ff579  True   \n",
       "2  b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  True   \n",
       "3  e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  True   \n",
       "4  4fe541af-912e-5a86-81a5-94c6d3891509  True   \n",
       "\n",
       "                                               text1  \\\n",
       "0  I shift a bit, warily letting my eyes dart fro...   \n",
       "1  I shift a bit, warily letting my eyes dart fro...   \n",
       "2  A single tear escaped me as I left. I did have...   \n",
       "3  \"Ja.\" Ludwig kept his gaze upon her, solidly. ...   \n",
       "4  And he did. Slowly, hesitantly...but coming fr...   \n",
       "\n",
       "                                               text2  \n",
       "0  \"All will become one with Russia,\" he said, al...  \n",
       "1  Suddenly, a piece of ice falls into the pit of...  \n",
       "2  got the Yang yoyo.\" Kimiko pulled the other ha...  \n",
       "3  SilverGray lll...YellowRagged llll...GrayMilli...  \n",
       "4  \"Let\"s go,\" Raimondo said and then started in ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.iloc[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21441"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_length = 0\n",
    "for i in range(len(df_combined)):\n",
    "    mean_length += len(df_combined.iloc[i, 2]) + len(df_combined.iloc[i, 3])\n",
    "\n",
    "mean_length /= len(df_combined) * 2\n",
    "mean_length = int(mean_length)\n",
    "mean_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, model_name, max_len=512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.data = df\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded_input_text1 = self.tokenizer(self.data.iloc[index, 2], max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoded_input_text2 = self.tokenizer(self.data.iloc[index, 3], max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            \"encoded_input_text1\": encoded_input_text1,\n",
    "            \"encoded_input_text2\": encoded_input_text2,\n",
    "            \"targets\": torch.tensor(int(self.data.iloc[index, 1]), dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# transformer without woth pairs\\nclass TransformerModel(nn.Module):\\n    def __init__(self, model_name):\\n        super(TransformerModel, self).__init__()\\n        # self.transformer = AutoModel.from_pretrained(model_name)\\n        self.dense1 = nn.Linear(768, 512)\\n        self.dropout = nn.Dropout(0.1)\\n        self.cosine = nn.CosineSimilarity(dim=1)\\n        self.dense = nn.Linear(1, 1)\\n        self.gelu = nn.GELU()\\n        self.sigmoid = nn.Sigmoid()\\n\\n    def forward(self, encoded_input_text1, encoded_input_text2):\\n        model_output_text1 = self.transformer(\\n            input_ids=encoded_input_text1['input_ids'][0, :, :],\\n            attention_mask=encoded_input_text1['attention_mask'],\\n        ).last_hidden_state[:, 0]\\n        model_output_text2 = self.transformer(\\n            input_ids=encoded_input_text2['input_ids'][0, :, :],\\n            attention_mask=encoded_input_text2['attention_mask'],\\n        ).last_hidden_state[:, 0]\\n\\n        x_a, x_b = self.dense1(model_output_text1), self.dense1(model_output_text2)\\n        x_a, x_b = self.gelu(self.dropout(x_a)), self.gelu(self.dropout(x_b))\\n        sem_sim = self.cosine(x_a, x_b)\\n        weighted_sem_sim = self.dense(sem_sim)\\n\\n        return self.sigmoid(weighted_sem_sim)\\n\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# transformer without woth pairs\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.dense1 = nn.Linear(768, 512)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.cosine = nn.CosineSimilarity(dim=1)\n",
    "        self.dense = nn.Linear(1, 1)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, encoded_input_text1, encoded_input_text2):\n",
    "        model_output_text1 = self.transformer(\n",
    "            input_ids=encoded_input_text1['input_ids'][0, :, :],\n",
    "            attention_mask=encoded_input_text1['attention_mask'],\n",
    "        ).last_hidden_state[:, 0]\n",
    "        model_output_text2 = self.transformer(\n",
    "            input_ids=encoded_input_text2['input_ids'][0, :, :],\n",
    "            attention_mask=encoded_input_text2['attention_mask'],\n",
    "        ).last_hidden_state[:, 0]\n",
    "\n",
    "        x_a, x_b = self.dense1(model_output_text1), self.dense1(model_output_text2)\n",
    "        x_a, x_b = self.gelu(self.dropout(x_a)), self.gelu(self.dropout(x_b))\n",
    "        sem_sim = self.cosine(x_a, x_b)\n",
    "        weighted_sem_sim = self.dense(sem_sim)\n",
    "\n",
    "        return self.sigmoid(weighted_sem_sim)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embedding_size=768, hidden_size=512):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Define the layers for the Transformer model\n",
    "        self.embedding = nn.Embedding(10000, embedding_size)  # Change 10000 to your vocabulary size\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=embedding_size, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=6)\n",
    "        \n",
    "        self.dense1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.cosine = nn.CosineSimilarity(dim=1)\n",
    "        self.dense = nn.Linear(1, 1)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, batch_encoding):\n",
    "        embedded_text1 = self.embedding(batch_encoding['input_ids'][:, 0, :])  # Extract input_ids\n",
    "        embedded_text2 = self.embedding(batch_encoding['input_ids'][:, 1, :])  # Extract input_ids\n",
    "        \n",
    "        encoded_text1 = self.transformer_encoder(embedded_text1)\n",
    "        encoded_text2 = self.transformer_encoder(embedded_text2)\n",
    "\n",
    "        model_output_text1 = encoded_text1[:, 0, :]  # Extract the first token's output\n",
    "        model_output_text2 = encoded_text2[:, 0, :]  # Extract the first token's output\n",
    "        \n",
    "        x_a, x_b = self.dense1(model_output_text1), self.dense1(model_output_text2)\n",
    "        x_a, x_b = self.gelu(self.dropout(x_a)), self.gelu(self.dropout(x_b))\n",
    "        sem_sim = self.cosine(x_a, x_b)\n",
    "        weighted_sem_sim = self.dense(sem_sim.unsqueeze(1))\n",
    "\n",
    "        return self.sigmoid(weighted_sem_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mio para comprobar que funciona y corre el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AnnaWegmann/Style-Embedding' # 'bert-base-uncased'  # Choose the appropriate pretrained model #'AnnaWegmann/Style-Embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_combined, test_size=0.2, random_state=42)\n",
    "train_dataset = CustomDataset(train_df, model_name, max_len=mean_length)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([29857, 11475, 43247, 22425,  4589, 49882, 36655,  8592, 34320, 33051,\n",
      "       ...\n",
      "       47191, 21962, 37194, 16850,  6265, 11284, 44732, 38158,   860, 15795],\n",
      "      dtype='int64', length=42098)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small test to see that everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerModel.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8903/353119778.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoded_input_text1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoded_input_text2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerModel.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# anna weinman style embeddings - hard negative mininng\n",
    "model = TransformerModel() #(model_name=model_name)\n",
    "model.train() # tell model we are going to train -> https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "\n",
    "for batch in train_data_loader:\n",
    "    x = model.forward(batch[\"encoded_input_text1\"], batch[\"encoded_input_text2\"])\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model\n",
    "\n",
    "See diapos a partir de la 152 y usar anotación de la diapos (ejemplo: bs_sl -> Batch size - Sequence Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your loss function (customize based on your task)\n",
    "criterion = nn.MSELoss()  # Example: Mean Squared Error\n",
    "\n",
    "# Define optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "train_df, val_df = train_test_split(df_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to compute accuracy or other evaluation metrics\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            pairs = batch['pairs']\n",
    "            fandoms = batch['fandoms']\n",
    "            y = batch['y']\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(pairs, fandoms)\n",
    "\n",
    "            # Calculate loss (customize based on your task)\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = CustomDataset(train_df, model_name)\n",
    "validate_dataset = CustomDataset(val_df, model_name)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Create a DataLoader for training and validation data\n",
    "    # You'll need to customize this part based on your dataset and preprocessing\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_data_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    for batch in train_data_loader:\n",
    "        # print(batch)\n",
    "        # print(type(batch))\n",
    "        \n",
    "        input_text1 = batch['encoded_input_text1']        # pairs\n",
    "        input_text2 = batch['encoded_input_text2']        # fandoms\n",
    "        targets = batch['targets']                        # y\n",
    "        \n",
    "        print(\"------------\")\n",
    "        print(f'Batch keys: {batch.keys()}')\n",
    "        print(\"------------\")\n",
    "        print(f'Batch belongs to type: {type(batch)}')\n",
    "        print(\"------------\")\n",
    "        print(input_text1)\n",
    "        print(input_text2)\n",
    "        print(targets)\n",
    "        print(\"------------\")\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model.forward(input_text1, input_text2)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(y_pred, targets)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss = evaluate(model, val_data_loader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_data_loader)}, Val Loss: {val_loss}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
