{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch transformers numpy pandas sentence-transformers -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bl1tty\\Documents\\Uni\\PAN-2023\\PAN-NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_FOLDER = \"PAN2020-authorship-verification\"\n",
    "DATASET1_TRAIN = \"pan20-authorship-verification-training-small/pan20-authorship-verification-training-small/pan20-authorship-verification-training-small-truth.jsonl\"\n",
    "DATASET2_TRAIN = \"pan20-authorship-verification-training-small/pan20-authorship-verification-training-small/pan20-authorship-verification-training-small.jsonl\"\n",
    "# DATASET1_TRAIN = \"pan20-authorship-verification-training-small-truth.jsonl\"\n",
    "# DATASET2_TRAIN = \"pan20-authorship-verification-training-small.jsonl\"\n",
    "FILE_PATH_1 = f'./{PARENT_FOLDER}/{DATASET1_TRAIN}'\n",
    "FILE_PATH_2 = f'./{PARENT_FOLDER}/{DATASET2_TRAIN}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_from_file (file_path : str) -> List:\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                parsed_data = json.loads(line)\n",
    "                data.append(parsed_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON: {e}\")\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "df_ground_truth = get_dataframe_from_file(FILE_PATH_1)\n",
    "df_inputs = get_dataframe_from_file(FILE_PATH_2)\n",
    "\n",
    "df_combined = pd.merge(df_ground_truth, df_inputs, on='id')\n",
    "\n",
    "######################\n",
    "#     CUIDADO!!!!!   #\n",
    "######################\n",
    "df_combined = df_combined.head(32)\n",
    "print(len(df_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>same</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6cced668-6e51-5212-873c-717f2bc91ce6</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c6c188a-db28-59aa-8c09-3d0f799ff579</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0cfa94f-c9ec-5aa5-8331-a5a249b664cf</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e6e86e73-9a7b-58f2-a652-a17b4a1bcabf</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fe541af-912e-5a86-81a5-94c6d3891509</td>\n",
       "      <td>True</td>\n",
       "      <td>[1446633, 1446633]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  same             authors\n",
       "0  6cced668-6e51-5212-873c-717f2bc91ce6  True  [1446633, 1446633]\n",
       "1  3c6c188a-db28-59aa-8c09-3d0f799ff579  True  [1446633, 1446633]\n",
       "2  b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  True  [1446633, 1446633]\n",
       "3  e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  True  [1446633, 1446633]\n",
       "4  4fe541af-912e-5a86-81a5-94c6d3891509  True  [1446633, 1446633]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52601"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_not_nulls(df: pd.DataFrame) -> None:\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicate_ids(df: pd.DataFrame) -> pd.Series:\n",
    "    # Find duplicate IDs\n",
    "    duplicate_ids = df[df.duplicated(subset=['id'], keep=False)]\n",
    "\n",
    "    # Calculate the sum of repetitions\n",
    "    sum_repetitions = len(duplicate_ids)\n",
    "\n",
    "    return sum_repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         0\n",
      "same       0\n",
      "authors    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_not_nulls(df_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         0\n",
      "fandoms    0\n",
      "pair       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_not_nulls(df_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "- Robust dataset: Separate pairs and with its fandoms. Use fandoms to generate new dataset of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert count_duplicate_ids(df_ground_truth) == count_duplicate_ids(df_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(df_combined) - len(df_inputs) == 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina la columna \"same\" ya que no da información relevante para el entrenamiento del modelo. Debido a que es una comparación entre dos ids que son las salidas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.drop(\"authors\", axis=1).drop(\"fandoms\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename \"authors\" to \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.rename(columns={'same': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "      <th>pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6cced668-6e51-5212-873c-717f2bc91ce6</td>\n",
       "      <td>True</td>\n",
       "      <td>[I shift a bit, warily letting my eyes dart fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c6c188a-db28-59aa-8c09-3d0f799ff579</td>\n",
       "      <td>True</td>\n",
       "      <td>[I shift a bit, warily letting my eyes dart fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0cfa94f-c9ec-5aa5-8331-a5a249b664cf</td>\n",
       "      <td>True</td>\n",
       "      <td>[A single tear escaped me as I left. I did hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e6e86e73-9a7b-58f2-a652-a17b4a1bcabf</td>\n",
       "      <td>True</td>\n",
       "      <td>[\"Ja.\" Ludwig kept his gaze upon her, solidly....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fe541af-912e-5a86-81a5-94c6d3891509</td>\n",
       "      <td>True</td>\n",
       "      <td>[And he did. Slowly, hesitantly...but coming f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     y  \\\n",
       "0  6cced668-6e51-5212-873c-717f2bc91ce6  True   \n",
       "1  3c6c188a-db28-59aa-8c09-3d0f799ff579  True   \n",
       "2  b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  True   \n",
       "3  e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  True   \n",
       "4  4fe541af-912e-5a86-81a5-94c6d3891509  True   \n",
       "\n",
       "                                                pair  \n",
       "0  [I shift a bit, warily letting my eyes dart fr...  \n",
       "1  [I shift a bit, warily letting my eyes dart fr...  \n",
       "2  [A single tear escaped me as I left. I did hav...  \n",
       "3  [\"Ja.\" Ludwig kept his gaze upon her, solidly....  \n",
       "4  [And he did. Slowly, hesitantly...but coming f...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   6cced668-6e51-5212-873c-717f2bc91ce6\n",
       "y                                                    True\n",
       "pair    [I shift a bit, warily letting my eyes dart fr...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[['text1', 'text2']] = df_combined['pair'].apply(pd.Series)\n",
    "df_combined = df_combined.drop(\"pair\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6cced668-6e51-5212-873c-717f2bc91ce6</td>\n",
       "      <td>True</td>\n",
       "      <td>I shift a bit, warily letting my eyes dart fro...</td>\n",
       "      <td>\"All will become one with Russia,\" he said, al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c6c188a-db28-59aa-8c09-3d0f799ff579</td>\n",
       "      <td>True</td>\n",
       "      <td>I shift a bit, warily letting my eyes dart fro...</td>\n",
       "      <td>Suddenly, a piece of ice falls into the pit of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0cfa94f-c9ec-5aa5-8331-a5a249b664cf</td>\n",
       "      <td>True</td>\n",
       "      <td>A single tear escaped me as I left. I did have...</td>\n",
       "      <td>got the Yang yoyo.\" Kimiko pulled the other ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e6e86e73-9a7b-58f2-a652-a17b4a1bcabf</td>\n",
       "      <td>True</td>\n",
       "      <td>\"Ja.\" Ludwig kept his gaze upon her, solidly. ...</td>\n",
       "      <td>SilverGray lll...YellowRagged llll...GrayMilli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fe541af-912e-5a86-81a5-94c6d3891509</td>\n",
       "      <td>True</td>\n",
       "      <td>And he did. Slowly, hesitantly...but coming fr...</td>\n",
       "      <td>\"Let\"s go,\" Raimondo said and then started in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     y  \\\n",
       "0  6cced668-6e51-5212-873c-717f2bc91ce6  True   \n",
       "1  3c6c188a-db28-59aa-8c09-3d0f799ff579  True   \n",
       "2  b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  True   \n",
       "3  e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  True   \n",
       "4  4fe541af-912e-5a86-81a5-94c6d3891509  True   \n",
       "\n",
       "                                               text1  \\\n",
       "0  I shift a bit, warily letting my eyes dart fro...   \n",
       "1  I shift a bit, warily letting my eyes dart fro...   \n",
       "2  A single tear escaped me as I left. I did have...   \n",
       "3  \"Ja.\" Ludwig kept his gaze upon her, solidly. ...   \n",
       "4  And he did. Slowly, hesitantly...but coming fr...   \n",
       "\n",
       "                                               text2  \n",
       "0  \"All will become one with Russia,\" he said, al...  \n",
       "1  Suddenly, a piece of ice falls into the pit of...  \n",
       "2  got the Yang yoyo.\" Kimiko pulled the other ha...  \n",
       "3  SilverGray lll...YellowRagged llll...GrayMilli...  \n",
       "4  \"Let\"s go,\" Raimondo said and then started in ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.iloc[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21336"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_length = 0\n",
    "for i in range(len(df_combined)):\n",
    "    mean_length += len(df_combined.iloc[i, 2]) + len(df_combined.iloc[i, 3])\n",
    "\n",
    "mean_length /= len(df_combined) * 2\n",
    "mean_length = int(mean_length)\n",
    "mean_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, model_name, max_len=512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.data = df\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded_input_text1 = self.tokenizer(self.data.iloc[index, 2], max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoded_input_text2 = self.tokenizer(self.data.iloc[index, 3], max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            \"encoded_input_text1\": encoded_input_text1,\n",
    "            \"encoded_input_text2\": encoded_input_text2,\n",
    "            \"targets\": torch.tensor(int(self.data.iloc[index, 1]), dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer without woth pairs\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, model_name, freeze_transformer):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        if freeze_transformer:\n",
    "            for param in self.transformer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.dense1 = nn.Linear(768, 512)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.cosine = nn.CosineSimilarity(dim=1)\n",
    "        self.dense = nn.Linear(1, 1)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, encoded_input_text1, encoded_input_text2):\n",
    "        model_output_text1 = self.transformer(\n",
    "            input_ids=encoded_input_text1['input_ids'][0, :, :],\n",
    "            attention_mask=encoded_input_text1['attention_mask'],\n",
    "        ).last_hidden_state[:, 0]\n",
    "        model_output_text2 = self.transformer(\n",
    "            input_ids=encoded_input_text2['input_ids'][0, :, :],\n",
    "            attention_mask=encoded_input_text2['attention_mask'],\n",
    "        ).last_hidden_state[:, 0]\n",
    "\n",
    "        x_a, x_b = self.dense1(model_output_text1), self.dense1(model_output_text2)\n",
    "        x_a, x_b = self.gelu(self.dropout(x_a)), self.gelu(self.dropout(x_b))\n",
    "        sem_sim = self.cosine(x_a, x_b)\n",
    "        sem_sim = sem_sim.view(sem_sim.size(0), -1)\n",
    "        weighted_sem_sim = self.dense(sem_sim)\n",
    "\n",
    "        return self.sigmoid(weighted_sem_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mio para comprobar que funciona y corre el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AnnaWegmann/Style-Embedding' # 'bert-base-uncased'  # Choose the appropriate pretrained model #'AnnaWegmann/Style-Embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_combined, test_size=0.2, random_state=42)\n",
    "train_dataset = CustomDataset(train_df, model_name, max_len=mean_length)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([25, 12,  0,  4, 16,  5, 13, 11, 23,  1,  2, 26,  3, 21, 27, 22, 18, 31,\n",
      "       20,  7, 10, 14, 28, 19,  6],\n",
      "      dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small test to see that everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2872]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# anna weinman style embeddings - hard negative mininng\n",
    "model = TransformerModel(model_name=model_name, freeze_transformer=True)\n",
    "model.train() # tell model we are going to train -> https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "\n",
    "for batch in train_data_loader:\n",
    "    x = model.forward(batch[\"encoded_input_text1\"], batch[\"encoded_input_text2\"])\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model\n",
    "\n",
    "See diapos a partir de la 152 y usar anotación de la diapos (ejemplo: bs_sl -> Batch size - Sequence Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to model_epoch_1.pt\n",
      "predictions (real): tensor([[0.4922],\n",
      "        [0.4922],\n",
      "        [0.4922],\n",
      "        [0.4922],\n",
      "        [0.4922],\n",
      "        [0.4922],\n",
      "        [0.4922]])\n",
      "predictions: tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "ground_truth: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch [1/10], Validation Loss: 0.2578928768634796, Accuracy: 0.0\n",
      "Model weights saved to model_epoch_2.pt\n",
      "predictions (real): tensor([[0.4982],\n",
      "        [0.4982],\n",
      "        [0.4982],\n",
      "        [0.4982],\n",
      "        [0.4982],\n",
      "        [0.4982],\n",
      "        [0.4982]])\n",
      "predictions: tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "ground_truth: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch [2/10], Validation Loss: 0.25181224942207336, Accuracy: 0.0\n",
      "Model weights saved to model_epoch_3.pt\n",
      "predictions (real): tensor([[0.5015],\n",
      "        [0.5015],\n",
      "        [0.5015],\n",
      "        [0.5015],\n",
      "        [0.5015],\n",
      "        [0.5015],\n",
      "        [0.5015]])\n",
      "predictions: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "ground_truth: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch [3/10], Validation Loss: 0.24846234917640686, Accuracy: 1.0\n",
      "Model weights saved to model_epoch_4.pt\n",
      "predictions (real): tensor([[0.5021],\n",
      "        [0.5021],\n",
      "        [0.5021],\n",
      "        [0.5021],\n",
      "        [0.5021],\n",
      "        [0.5021],\n",
      "        [0.5021]])\n",
      "predictions: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "ground_truth: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch [4/10], Validation Loss: 0.2478867918252945, Accuracy: 1.0\n",
      "Model weights saved to model_epoch_5.pt\n",
      "predictions (real): tensor([[0.5001],\n",
      "        [0.5001],\n",
      "        [0.5001],\n",
      "        [0.5001],\n",
      "        [0.5001],\n",
      "        [0.5001],\n",
      "        [0.5001]])\n",
      "predictions: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "ground_truth: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch [5/10], Validation Loss: 0.24990198016166687, Accuracy: 1.0\n",
      "Model weights saved to model_epoch_6.pt\n",
      "predictions (real): tensor([[0.5048],\n",
      "        [0.5048],\n",
      "        [0.5048],\n",
      "        [0.5048],\n",
      "        [0.5048],\n",
      "        [0.5048],\n",
      "        [0.5048]])\n",
      "predictions: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "ground_truth: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch [6/10], Validation Loss: 0.24522528052330017, Accuracy: 1.0\n",
      "Model weights saved to model_epoch_7.pt\n",
      "predictions (real): tensor([[0.5062],\n",
      "        [0.5062],\n",
      "        [0.5062],\n",
      "        [0.5062],\n",
      "        [0.5062],\n",
      "        [0.5062],\n",
      "        [0.5062]])\n",
      "predictions: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "ground_truth: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch [7/10], Validation Loss: 0.2438202202320099, Accuracy: 1.0\n",
      "Model weights saved to model_epoch_8.pt\n",
      "predictions (real): tensor([[0.5035],\n",
      "        [0.5035],\n",
      "        [0.5035],\n",
      "        [0.5035],\n",
      "        [0.5035],\n",
      "        [0.5035],\n",
      "        [0.5035]])\n",
      "predictions: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "ground_truth: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch [8/10], Validation Loss: 0.24648751318454742, Accuracy: 1.0\n",
      "Model weights saved to model_epoch_9.pt\n",
      "predictions (real): tensor([[0.5062],\n",
      "        [0.5062],\n",
      "        [0.5062],\n",
      "        [0.5062],\n",
      "        [0.5062],\n",
      "        [0.5062],\n",
      "        [0.5062]])\n",
      "predictions: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "ground_truth: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch [9/10], Validation Loss: 0.24379539489746094, Accuracy: 1.0\n",
      "Model weights saved to model_epoch_10.pt\n",
      "predictions (real): tensor([[0.5072],\n",
      "        [0.5072],\n",
      "        [0.5072],\n",
      "        [0.5072],\n",
      "        [0.5072],\n",
      "        [0.5072],\n",
      "        [0.5072]])\n",
      "predictions: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "ground_truth: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch [10/10], Validation Loss: 0.2428467571735382, Accuracy: 1.0\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(model_name=model_name, freeze_transformer=True)\n",
    "\n",
    "# Define your loss function (customize based on your task)\n",
    "criterion = nn.MSELoss()  # Example: Mean Squared Error\n",
    "\n",
    "# Define optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "train_df, val_df = train_test_split(df_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            encoded_input_text1 = batch['encoded_input_text1']\n",
    "            encoded_input_text2 = batch['encoded_input_text2']\n",
    "            targets = batch['targets']\n",
    "\n",
    "            y_pred = model.forward(encoded_input_text1, encoded_input_text2)\n",
    "\n",
    "            if targets.dim() == 1:\n",
    "                targets = targets.view(-1, 1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = (y_pred > 0.5).float()  # Assuming a binary classification task\n",
    "            correct_predictions += (predictions == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "\n",
    "    print(f\"predictions (real): {y_pred}\")\n",
    "    print(f\"predictions: {predictions}\")\n",
    "    print(f\"ground_truth: {targets}\")\n",
    "\n",
    "    return average_loss, accuracy\n",
    "\n",
    "\n",
    "def training_step(encoded_input_text1, encoded_input_text2, targets, model, optimizer, criterion):\n",
    "    # !!!! necessary to set the model to training mode before\n",
    "    \n",
    "    # forward pass\n",
    "    y_pred = model.forward(encoded_input_text1, encoded_input_text2)\n",
    "    \n",
    "    if targets.dim() == 1:\n",
    "        targets = targets.view(-1, 1)\n",
    "\n",
    "    loss = criterion(y_pred, targets)\n",
    "    \n",
    "    # baccpropagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = CustomDataset(train_df, model_name)\n",
    "validate_dataset = CustomDataset(val_df, model_name)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(train_data_loader):\n",
    "        input_text1 = batch['encoded_input_text1']\n",
    "        input_text2 = batch['encoded_input_text2']\n",
    "        targets = batch['targets']\n",
    "\n",
    "        loss = training_step(input_text1, input_text2, targets, model, optimizer, criterion)\n",
    "        running_loss += loss\n",
    "\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                  f\"Step [{i + 1}/{len(train_data_loader)}], \"\n",
    "                  f\"Loss: {running_loss / 100}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Save the model weights after each epoch\n",
    "    checkpoint_path = f\"model_epoch_{epoch + 1}.pt\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Model weights saved to {checkpoint_path}\")\n",
    "\n",
    "    # Evaluate the model on the validation set after each epoch\n",
    "    val_loss, val_accuracy = evaluate(model, val_data_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss}, Accuracy: {val_accuracy}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUEDA:\n",
    "\n",
    "1. El validate-test serían pasar en un bucle el forward del otro zip y capturar resultados para conseguir las métricas\n",
    "2. Fine tunning (mínimo)\n",
    "3. Escribir cosas\n",
    "\n",
    "PD: Quitar el print de los shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = f\"{PARENT_FOLDER}/pan20-authorship-verification-test/pan20-authorship-verification-test\"\n",
    "VALUES_FILE = \"pan20-authorship-verification-test.jsonl\"\n",
    "GROUND_TRUTH = \"pan20-authorship-verification-test-truth.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = get_dataframe_from_file(f\"{FOLDER}/{GROUND_TRUTH}\")\n",
    "df_inputs = get_dataframe_from_file(f\"{FOLDER}/{VALUES_FILE}\")\n",
    "\n",
    "df_combined_val = pd.merge(df_ground_truth, df_inputs, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_val = df_combined.drop(\"authors\", axis=1).drop(\"fandoms\", axis=1)\n",
    "df_combined_val = df_combined_val.rename(columns={'same': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "      <th>pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c04fdf1e-ddf5-5542-96e7-13ce18cae176</td>\n",
       "      <td>True</td>\n",
       "      <td>[\"Calm down, Nicolas. You don\"t wanna do somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49dc4cae-3d32-5b4d-b240-a080a1dbb659</td>\n",
       "      <td>False</td>\n",
       "      <td>[\"Squall!?\" Zell was panicking. Squall was mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f326fe7c-fc10-566f-a70f-0f36e3f92399</td>\n",
       "      <td>False</td>\n",
       "      <td>[\"Just talk to the first girl you bump into an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16daa0d1-61b8-5650-b7ee-5e265bd40910</td>\n",
       "      <td>True</td>\n",
       "      <td>[\"I\"ll be fine,\" Alfred said, his grin not wav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08b536a8-4fed-5f62-97bb-e57f79e841d2</td>\n",
       "      <td>False</td>\n",
       "      <td>[dominated by a huge desk. Behind the desk, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id      y  \\\n",
       "0  c04fdf1e-ddf5-5542-96e7-13ce18cae176   True   \n",
       "1  49dc4cae-3d32-5b4d-b240-a080a1dbb659  False   \n",
       "2  f326fe7c-fc10-566f-a70f-0f36e3f92399  False   \n",
       "3  16daa0d1-61b8-5650-b7ee-5e265bd40910   True   \n",
       "4  08b536a8-4fed-5f62-97bb-e57f79e841d2  False   \n",
       "\n",
       "                                                pair  \n",
       "0  [\"Calm down, Nicolas. You don\"t wanna do somet...  \n",
       "1  [\"Squall!?\" Zell was panicking. Squall was mov...  \n",
       "2  [\"Just talk to the first girl you bump into an...  \n",
       "3  [\"I\"ll be fine,\" Alfred said, his grin not wav...  \n",
       "4  [dominated by a huge desk. Behind the desk, an...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   c04fdf1e-ddf5-5542-96e7-13ce18cae176\n",
       "y                                                    True\n",
       "pair    [\"Calm down, Nicolas. You don\"t wanna do somet...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_val.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_val[['text1', 'text2']] = df_combined_val['pair'].apply(pd.Series)\n",
    "df_combined_val = df_combined_val.drop(\"pair\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_epoch_10.pt\"))\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_dataset = CustomDataset(train_df, model_name)\n",
    "validate_dataset = CustomDataset(val_df, model_name)\n",
    "\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_data_loader):\n",
    "        input_text1 = batch['encoded_input_text1']\n",
    "        input_text2 = batch['encoded_input_text2']\n",
    "        targets = batch['targets']\n",
    "\n",
    "        outputs = model.forward(input_text1, input_text2)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {100 * accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
